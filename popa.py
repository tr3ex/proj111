# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.model_selection import train_test_split, cross_val_score
# from sklearn.preprocessing import StandardScaler, RobustScaler
# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor
# from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
# from sklearn.svm import SVR
# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
# import xgboost as xgb
# import warnings
# warnings.filterwarnings('ignore')

# # =============================================================================
# # 2.1. –í–´–ë–û–† –ò –û–ü–ò–°–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê
# # =============================================================================

# print("=" * 60)
# print("2.1. –í–´–ë–û–† –ò –û–ü–ò–°–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê")
# print("=" * 60)

# # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
# df = pd.read_csv('kp_all_movies_cleanedd.csv')
# print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}")

# # –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
# print("\n–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
# print(df.head())

# print("\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–∞–Ω–Ω—ã—Ö:")
# print(df.info())

# print("\n–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
# print(df.describe())

# # –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
# if 'movie_year' in df.columns:
#     print(f"\n–í—Ä–µ–º–µ–Ω–Ω–æ–π –ø–µ—Ä–∏–æ–¥ –æ—Ö–≤–∞—Ç–∞: {df['movie_year'].min()} - {df['movie_year'].max()} –≥–≥.")
#     print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏—è–º:")
#     decade_counts = (df['movie_year'] // 10 * 10).value_counts().sort_index()
#     for decade, count in decade_counts.items():
#         print(f"  {decade}-–µ: {count} —Ñ–∏–ª—å–º–æ–≤")

# # –ê–Ω–∞–ª–∏–∑ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤
# if 'kp_rating' in df.columns:
#     print(f"\n–ê–Ω–∞–ª–∏–∑ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –ö–∏–Ω–æ–ü–æ–∏—Å–∫:")
#     print(f"  –î–∏–∞–ø–∞–∑–æ–Ω: {df['kp_rating'].min():.2f} - {df['kp_rating'].max():.2f}")
#     print(f"  –°—Ä–µ–¥–Ω–µ–µ: {df['kp_rating'].mean():.2f} ¬± {df['kp_rating'].std():.2f}")
#     print(f"  –ú–µ–¥–∏–∞–Ω–∞: {df['kp_rating'].median():.2f}")

# # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–¥–µ–ª–∞ 2.1
# fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# # 1. –î–∏–∞–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–∏–ª—å–º–æ–≤ –ø–æ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏—è–º
# if 'movie_year' in df.columns:
#     df_decade = (df['movie_year'] // 10 * 10).value_counts().sort_index()
#     bars = axes[0,0].bar(df_decade.index, df_decade.values, color='skyblue', alpha=0.7, edgecolor='black')
#     axes[0,0].set_xlabel('–î–µ—Å—è—Ç–∏–ª–µ—Ç–∏–µ')
#     axes[0,0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–ª—å–º–æ–≤')
#     axes[0,0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∏–ª—å–º–æ–≤ –ø–æ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏—è–º')
#     axes[0,0].grid(True, alpha=0.3)
#     # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–ø–∏—Å–∏ –∑–Ω–∞—á–µ–Ω–∏–π
#     for bar in bars:
#         height = bar.get_height()
#         axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 5,
#                       f'{int(height)}', ha='center', va='bottom', fontsize=9)

# # 2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –ö–∏–Ω–æ–ü–æ–∏—Å–∫
# if 'kp_rating' in df.columns:
#     axes[0,1].hist(df['kp_rating'].dropna(), bins=30, color='lightgreen', alpha=0.7, edgecolor='black')
#     axes[0,1].set_xlabel('–†–µ–π—Ç–∏–Ω–≥ –ö–∏–Ω–æ–ü–æ–∏—Å–∫')
#     axes[0,1].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
#     axes[0,1].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –ö–∏–Ω–æ–ü–æ–∏—Å–∫')
#     axes[0,1].grid(True, alpha=0.3)

# # 3. –ê–Ω–∞–ª–∏–∑ –∂–∞–Ω—Ä–æ–≤
# if 'genres' in df.columns:
#     all_genres = []
#     for genres in df['genres'].dropna():
#         if isinstance(genres, str):
#             all_genres.extend([genre.strip() for genre in genres.split(',')])
#     top_genres = pd.Series(all_genres).value_counts().head(10)
#     bars = axes[1,0].barh(range(len(top_genres)), top_genres.values, color='lightcoral', alpha=0.7, edgecolor='black')
#     axes[1,0].set_yticks(range(len(top_genres)))
#     axes[1,0].set_yticklabels(top_genres.index)
#     axes[1,0].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–ª—å–º–æ–≤')
#     axes[1,0].set_title('–¢–æ–ø-10 –∂–∞–Ω—Ä–æ–≤')
#     # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–ø–∏—Å–∏ –∑–Ω–∞—á–µ–Ω–∏–π
#     for i, bar in enumerate(bars):
#         width = bar.get_width()
#         axes[1,0].text(width + 5, bar.get_y() + bar.get_height()/2.,
#                       f'{int(width)}', ha='left', va='center', fontsize=9)

# # 4. –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω
# if 'countries' in df.columns:
#     all_countries = []
#     for countries in df['countries'].dropna():
#         if isinstance(countries, str):
#             all_countries.extend([country.strip() for country in countries.split(',')])
#     top_countries = pd.Series(all_countries).value_counts().head(10)
#     bars = axes[1,1].barh(range(len(top_countries)), top_countries.values, color='gold', alpha=0.7, edgecolor='black')
#     axes[1,1].set_yticks(range(len(top_countries)))
#     axes[1,1].set_yticklabels(top_countries.index)
#     axes[1,1].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–ª—å–º–æ–≤')
#     axes[1,1].set_title('–¢–æ–ø-10 —Å—Ç—Ä–∞–Ω –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞')
#     # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–ø–∏—Å–∏ –∑–Ω–∞—á–µ–Ω–∏–π
#     for i, bar in enumerate(bars):
#         width = bar.get_width()
#         axes[1,1].text(width + 5, bar.get_y() + bar.get_height()/2.,
#                       f'{int(width)}', ha='left', va='center', fontsize=9)

# plt.tight_layout()
# plt.show()

# # =============================================================================
# # 2.2. –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –ò –†–ê–ó–í–ï–î–û–ß–ù–´–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• (EDA)
# # =============================================================================

# print("\n" + "=" * 60)
# print("2.2. –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –ò –†–ê–ó–í–ï–î–û–ß–ù–´–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• (EDA)")
# print("=" * 60)

# # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
# print("\n–ê–ù–ê–õ–ò–ó –ü–†–û–ü–£–©–ï–ù–ù–´–• –ó–ù–ê–ß–ï–ù–ò–ô:")
# missing_data = df.isnull().sum()
# missing_percent = (df.isnull().sum() / len(df)) * 100
# missing_info = pd.DataFrame({
#     '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤': missing_data,
#     '–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤': missing_percent
# })
# missing_info = missing_info[missing_info['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤'] > 0]
# print(missing_info)

# # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
# if len(missing_info) > 0:
#     plt.figure(figsize=(12, 6))
#     missing_plot_data = missing_info.sort_values('–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤', ascending=False)
#     bars = plt.bar(missing_plot_data.index, missing_plot_data['–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤'], 
#                   color='salmon', alpha=0.7, edgecolor='black')
#     plt.xticks(rotation=45, ha='right')
#     plt.xlabel('–ü—Ä–∏–∑–Ω–∞–∫–∏')
#     plt.ylabel('–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤ (%)')
#     plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º')
#     plt.grid(True, alpha=0.3)
    
#     # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
#     for bar in bars:
#         height = bar.get_height()
#         plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,
#                 f'{height:.1f}%', ha='center', va='bottom', fontsize=9)
    
#     plt.tight_layout()
#     plt.show()
# else:
#     print("–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")

# # –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
# def preprocess_data_improved(df):
#     df_clean = df.copy()
    
#     # –ë–∞–∑–æ–≤—ã–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤
#     numeric_columns = ['kp_rating', 'movie_duration', 'kp_rating_count', 'movie_year', 
#                      'imdb_rating', 'imdb_rating_count', 'critics_rating']
    
#     for col in numeric_columns:
#         if col in df_clean.columns:
#             df_clean[col] = df_clean[col].fillna(df_clean[col].median())
    
#     # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±—é–¥–∂–µ—Ç–∞
#     def parse_budget_simple(budget_str):
#         if pd.isna(budget_str):
#             return np.nan
#         try:
#             budget_str = str(budget_str).replace(' ', '').replace('$', '').replace('‚Ç¨', '').replace('¬£', '')
#             budget_str = budget_str.replace('¬•', '').replace('INR', '').replace('CAD', '').replace('AUD', '')
#             budget_str = budget_str.replace('–º–ª–Ω', '').replace('million', '')
#             return float(budget_str) * 1000000 if any(x in str(budget_str).lower() for x in ['–º–ª–Ω', 'million']) else float(budget_str)
#         except:
#             return np.nan
    
#     if 'budget' in df_clean.columns:
#         df_clean['budget_parsed'] = df_clean['budget'].apply(parse_budget_simple)
#         df_clean['budget_parsed'] = df_clean['budget_parsed'].fillna(df_clean['budget_parsed'].median())
#         df_clean['log_budget'] = np.log1p(df_clean['budget_parsed'])
    
#     # –û—Å–Ω–æ–≤–Ω—ã–µ –∂–∞–Ω—Ä—ã
#     main_genres = ['–¥—Ä–∞–º–∞', '–∫–æ–º–µ–¥–∏—è', '–±–æ–µ–≤–∏–∫', '—Ç—Ä–∏–ª–ª–µ—Ä', '–º–µ–ª–æ–¥—Ä–∞–º–∞', '—Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞', '—É–∂–∞—Å—ã']
#     if 'genres' in df_clean.columns:
#         for genre in main_genres:
#             df_clean[f'genre_{genre}'] = df_clean['genres'].apply(
#                 lambda x: 1 if isinstance(x, str) and genre in x.lower() else 0
#             )
    
#     # –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç—Ä–∞–Ω—ã
#     main_countries = ['–°–®–ê', '–í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏—è', '–§—Ä–∞–Ω—Ü–∏—è', '–ì–µ—Ä–º–∞–Ω–∏—è']
#     if 'countries' in df_clean.columns:
#         for country in main_countries:
#             df_clean[f'country_{country}'] = df_clean['countries'].apply(
#                 lambda x: 1 if isinstance(x, str) and country in x else 0
#             )
    
#     # –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
#     df_clean['has_critics_rating'] = (~df_clean['critics_rating'].isna()).astype(int)
#     if 'kp_rating_count' in df_clean.columns:
#         df_clean['log_kp_rating_count'] = np.log1p(df_clean['kp_rating_count'])
#     if 'imdb_rating_count' in df_clean.columns:
#         df_clean['log_imdb_rating_count'] = np.log1p(df_clean['imdb_rating_count'])
    
#     # –£–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é
#     columns_to_drop = ['name_rus', 'name_eng', 'genres', 'countries', 'budget', 
#                       'movie_id', 'kp_rating_count', 'imdb_rating_count', 'budget_parsed']
#     for col in columns_to_drop:
#         if col in df_clean.columns:
#             df_clean = df_clean.drop(col, axis=1)
    
#     return df_clean

# # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
# df_processed = preprocess_data_improved(df)
# print(f"–†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {df_processed.shape}")
# print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(df_processed.columns)}")
# print(f"–ü—Ä–∏–∑–Ω–∞–∫–∏: {list(df_processed.columns)}")

# # EDA –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
# fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# # 1. –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
# numeric_columns = df_processed.select_dtypes(include=[np.number]).columns
# if len(numeric_columns) > 1:
#     # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 8 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
#     cols_to_plot = numeric_columns[:min(8, len(numeric_columns))]
#     correlation_matrix = df_processed[cols_to_plot].corr()
    
#     im = axes[0,0].imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)
#     axes[0,0].set_xticks(range(len(cols_to_plot)))
#     axes[0,0].set_yticks(range(len(cols_to_plot)))
#     axes[0,0].set_xticklabels([col[:15] + '...' if len(col) > 15 else col for col in cols_to_plot], 
#                              rotation=45, ha='right', fontsize=9)
#     axes[0,0].set_yticklabels([col[:15] + '...' if len(col) > 15 else col for col in cols_to_plot], 
#                              fontsize=9)
#     axes[0,0].set_title('–ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
    
#     # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
#     for i in range(len(cols_to_plot)):
#         for j in range(len(cols_to_plot)):
#             text_color = 'white' if abs(correlation_matrix.iloc[i, j]) > 0.5 else 'black'
#             axes[0,0].text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',
#                          ha="center", va="center", color=text_color, fontsize=8)
    
#     plt.colorbar(im, ax=axes[0,0])

# # 2. Box-plot –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ñ–∏–ª—å–º–æ–≤
# if 'movie_duration' in df_processed.columns:
#     data_to_plot = df_processed['movie_duration'].dropna()
#     if len(data_to_plot) > 0:
#         box_plot = axes[0,1].boxplot(data_to_plot, patch_artist=True)
#         box_plot['boxes'][0].set_facecolor('lightblue')
#         box_plot['boxes'][0].set_alpha(0.7)
#         axes[0,1].set_ylabel('–ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–º–∏–Ω—É—Ç—ã)')
#         axes[0,1].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ñ–∏–ª—å–º–æ–≤')
#         axes[0,1].grid(True, alpha=0.3)

# # 3. Scatter plot: –≥–æ–¥ –≤—ã–ø—É—Å–∫–∞ vs —Ä–µ–π—Ç–∏–Ω–≥
# if 'movie_year' in df_processed.columns and 'kp_rating' in df_processed.columns:
#     valid_data = df_processed[['movie_year', 'kp_rating']].dropna()
#     if len(valid_data) > 0:
#         axes[1,0].scatter(valid_data['movie_year'], valid_data['kp_rating'], alpha=0.5, s=20)
#         axes[1,0].set_xlabel('–ì–æ–¥ –≤—ã–ø—É—Å–∫–∞')
#         axes[1,0].set_ylabel('KP –†–µ–π—Ç–∏–Ω–≥')
#         axes[1,0].set_title('–†–µ–π—Ç–∏–Ω–≥ –ø–æ –≥–æ–¥–∞–º')
#         axes[1,0].grid(True, alpha=0.3)

# # 4. Scatter plot: IMDB vs KP —Ä–µ–π—Ç–∏–Ω–≥
# if 'imdb_rating' in df_processed.columns and 'kp_rating' in df_processed.columns:
#     valid_data = df_processed[['imdb_rating', 'kp_rating']].dropna()
#     if len(valid_data) > 0:
#         axes[1,1].scatter(valid_data['imdb_rating'], valid_data['kp_rating'], alpha=0.5, s=20)
#         axes[1,1].set_xlabel('IMDB –†–µ–π—Ç–∏–Ω–≥')
#         axes[1,1].set_ylabel('KP –†–µ–π—Ç–∏–Ω–≥')
#         axes[1,1].set_title('KP vs IMDB —Ä–µ–π—Ç–∏–Ω–≥')
#         axes[1,1].grid(True, alpha=0.3)

# plt.tight_layout()
# plt.show()

# # =============================================================================
# # 2.3-2.4. –í–´–ë–û–† –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í –ò –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –°–ò–°–¢–ï–ú–´
# # =============================================================================

# print("\n" + "=" * 60)
# print("2.3-2.4. –í–´–ë–û–† –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í –ò –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –°–ò–°–¢–ï–ú–´")
# print("=" * 60)

# print("""
# –í–´–ë–†–ê–ù–ù–´–ï –ò–ù–°–¢–†–£–ú–ï–ù–¢–´:
# ‚Ä¢ Pandas, NumPy - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
# ‚Ä¢ Scikit-learn - –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã ML
# ‚Ä¢ XGBoost - –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥
# ‚Ä¢ Matplotlib, Seaborn - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è

# –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –°–ò–°–¢–ï–ú–´:
# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
# 2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞
# 3. Feature engineering
# 4. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –≤—ã–±–æ—Ä–∫–∏
# 5. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
# 6. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
# 7. –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ—Ü–µ–Ω–∫–∞
# 8. –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
# """)

# # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
# fig, ax = plt.subplots(figsize=(12, 6))
# stages = ["–ó–∞–≥—Ä—É–∑–∫–∞\n–¥–∞–Ω–Ω—ã—Ö", "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞", "Feature\nEngineering", 
#           "–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ\n–¥–∞–Ω–Ω—ã—Ö", "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ", "–û–±—É—á–µ–Ω–∏–µ\n–º–æ–¥–µ–ª–µ–π", 
#           "–í–∞–ª–∏–¥–∞—Ü–∏—è", "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è"]

# y_pos = np.arange(len(stages))
# colors = plt.cm.Set3(np.linspace(0, 1, len(stages)))

# bars = ax.barh(y_pos, [1]*len(stages), color=colors, alpha=0.7, edgecolor='black')
# ax.set_yticks(y_pos)
# ax.set_yticklabels(stages, fontsize=10)
# ax.set_xlabel('–≠—Ç–∞–ø—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏')
# ax.set_title('–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è')
# ax.grid(True, alpha=0.3)

# # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–µ–ª–∫–∏
# for i in range(len(stages)-1):
#     ax.annotate('‚Üí', xy=(0.5, y_pos[i] - 0.4), xytext=(0.5, y_pos[i] - 0.1),
#                 arrowprops=dict(arrowstyle='->', lw=2, color='black'),
#                 ha='center', va='center', fontsize=16, xycoords='data')

# plt.tight_layout()
# plt.show()

# # =============================================================================
# # 2.5. –†–ï–ê–õ–ò–ó–ê–¶–ò–Ø –ò –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò
# # =============================================================================

# print("\n" + "=" * 60)
# print("2.5. –†–ï–ê–õ–ò–ó–ê–¶–ò–Ø –ò –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò")
# print("=" * 60)

# # –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã—Å–æ–∫–æ–∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
# def remove_highly_correlated_features(df, threshold=0.85):
#     corr_matrix = df.corr().abs()
#     upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
#     to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]
#     if to_drop:
#         print(f"–£–¥–∞–ª—è–µ–º –≤—ã—Å–æ–∫–æ–∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {to_drop}")
#         return df.drop(to_drop, axis=1)
#     else:
#         print("–í—ã—Å–æ–∫–æ–∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
#         return df

# df_processed = remove_highly_correlated_features(df_processed, threshold=0.8)
# print(f"–†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {df_processed.shape}")

# # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
# if 'kp_rating' not in df_processed.columns:
#     raise ValueError("–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'kp_rating' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞–Ω–Ω—ã—Ö")

# feature_columns = [col for col in df_processed.columns if col != 'kp_rating']
# X = df_processed[feature_columns]
# y = df_processed['kp_rating']

# print(f"–§–∏–Ω–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_columns)}")
# print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å X: {X.shape}, y: {y.shape}")

# # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
# if len(X) == 0 or len(y) == 0:
#     raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")

# # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
# try:
#     y_bins = pd.cut(y, bins=min(5, len(y.unique())), labels=False)
#     X_temp, X_test, y_temp, y_test = train_test_split(
#         X, y, test_size=0.2, random_state=42, stratify=y_bins
#     )
#     X_train, X_val, y_train, y_val = train_test_split(
#         X_temp, y_temp, test_size=0.25, random_state=42, 
#         stratify=pd.cut(y_temp, bins=min(5, len(y_temp.unique())), labels=False)
#     )
# except ValueError as e:
#     print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏: {e}")
#     # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –µ—Å–ª–∏ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
#     X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#     X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)

# print(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape}")
# print(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_val.shape}")
# print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape}")

# # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
# scaler = RobustScaler()
# X_train_scaled = scaler.fit_transform(X_train)
# X_val_scaled = scaler.transform(X_val)
# X_test_scaled = scaler.transform(X_test)

# # –§—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏
# def evaluate_model_improved(model, X_train, X_val, y_train, y_val, model_name):
#     """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""
    
#     try:
#         cv_scores = cross_val_score(model, X_train, y_train, cv=min(5, len(y_train)), scoring='r2')
#     except:
#         cv_scores = [0]  # –ï—Å–ª–∏ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
        
#     model.fit(X_train, y_train)
#     y_train_pred = model.predict(X_train)
#     y_val_pred = model.predict(X_val)
    
#     train_r2 = r2_score(y_train, y_train_pred)
#     val_r2 = r2_score(y_val, y_val_pred)
#     train_mae = mean_absolute_error(y_train, y_train_pred)
#     val_mae = mean_absolute_error(y_val, y_val_pred)
#     train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
#     val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
    
#     r2_gap = train_r2 - val_r2
#     relative_overfit = (train_r2 - val_r2) / abs(train_r2) if train_r2 != 0 else 0
    
#     results = {
#         'model': model_name,
#         'cv_r2_mean': np.mean(cv_scores),
#         'cv_r2_std': np.std(cv_scores),
#         'train_r2': train_r2,
#         'val_r2': val_r2,
#         'r2_gap': r2_gap,
#         'relative_overfit': relative_overfit,
#         'train_mae': train_mae,
#         'val_mae': val_mae,
#         'train_rmse': train_rmse,
#         'val_rmse': val_rmse,
#         'is_overfit': relative_overfit > 0.3 and r2_gap > 0.1
#     }
    
#     return results, model

# # –ú–æ–¥–µ–ª–∏ —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
# models_regularized = {
#     'Ridge': Ridge(alpha=10.0, random_state=42),
#     'Lasso': Lasso(alpha=1.0, random_state=42, max_iter=2000),
#     'ElasticNet': ElasticNet(alpha=0.5, l1_ratio=0.5, random_state=42, max_iter=2000),
#     'RandomForest': RandomForestRegressor(
#         n_estimators=50,
#         max_depth=8,
#         min_samples_split=20,
#         min_samples_leaf=10,
#         max_features='sqrt',
#         random_state=42,
#         n_jobs=-1
#     ),
#     'GradientBoosting': GradientBoostingRegressor(
#         n_estimators=100,
#         max_depth=4,
#         learning_rate=0.05,
#         min_samples_split=15,
#         min_samples_leaf=10,
#         subsample=0.8,
#         random_state=42
#     ),
#     'XGBoost': xgb.XGBRegressor(
#         n_estimators=100,
#         max_depth=4,
#         learning_rate=0.05,
#         reg_alpha=1.0,
#         reg_lambda=1.0,
#         subsample=0.8,
#         colsample_bytree=0.8,
#         random_state=42,
#         n_jobs=-1
#     )
# }

# print("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π...")
# results_list = []
# trained_models = {}

# for name, model in models_regularized.items():
#     print(f"–û–±—É—á–µ–Ω–∏–µ {name}...")
#     try:
#         results, trained_model = evaluate_model_improved(
#             model, X_train_scaled, X_val_scaled, y_train, y_val, name
#         )
#         results_list.append(results)
#         trained_models[name] = trained_model
#         status = "‚ö†Ô∏è –ü–ï–†–ï–û–ë–£–ß–ï–ù–ê" if results['is_overfit'] else "‚úÖ –ù–û–†–ú–ê"
#         print(f"  {status} | Train R2: {results['train_r2']:.3f} | Val R2: {results['val_r2']:.3f}")
#     except Exception as e:
#         print(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")

# # =============================================================================
# # 2.6. –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ú–û–î–ï–õ–ò
# # =============================================================================

# print("\n" + "=" * 60)
# print("2.6. –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ú–û–î–ï–õ–ò")
# print("=" * 60)

# if not results_list:
#     raise ValueError("–ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞")

# # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
# results_df = pd.DataFrame(results_list)
# print("\n–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ú–û–î–ï–õ–ï–ô:")
# display_cols = ['model', 'train_r2', 'val_r2', 'r2_gap', 'relative_overfit', 'is_overfit']
# print(results_df[display_cols].round(4))

# # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
# fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ R2
# colors = ['red' if overfit else 'green' for overfit in results_df['is_overfit']]
# x_pos = np.arange(len(results_df))

# bars1 = axes[0,0].bar(x_pos - 0.2, results_df['train_r2'], 0.4, label='Train R2', alpha=0.7, color='lightblue')
# bars2 = axes[0,0].bar(x_pos + 0.2, results_df['val_r2'], 0.4, label='Val R2', alpha=0.7, color=colors)
# axes[0,0].set_xlabel('–ú–æ–¥–µ–ª–∏')
# axes[0,0].set_ylabel('R2 Score')
# axes[0,0].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ R2 (–ö—Ä–∞—Å–Ω—ã–π = –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ)')
# axes[0,0].set_xticks(x_pos)
# axes[0,0].set_xticklabels(results_df['model'], rotation=45, ha='right')
# axes[0,0].legend()
# axes[0,0].grid(True, alpha=0.3)

# # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
# for bars in [bars1, bars2]:
#     for bar in bars:
#         height = bar.get_height()
#         axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 0.01,
#                       f'{height:.3f}', ha='center', va='bottom', fontsize=8)

# # 2. Gap –∞–Ω–∞–ª–∏–∑
# bars = axes[0,1].bar(results_df['model'], results_df['r2_gap'], color=colors, alpha=0.7, edgecolor='black')
# axes[0,1].axhline(y=0.1, color='red', linestyle='--', label='–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ø–æ—Ä–æ–≥')
# axes[0,1].set_xlabel('–ú–æ–¥–µ–ª–∏')
# axes[0,1].set_ylabel('R2 Gap (Train - Val)')
# axes[0,1].set_title('–†–∞–∑—Ä—ã–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏')
# axes[0,1].set_xticklabels(results_df['model'], rotation=45, ha='right')
# axes[0,1].legend()
# axes[0,1].grid(True, alpha=0.3)

# # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
# for bar in bars:
#     height = bar.get_height()
#     axes[0,1].text(bar.get_x() + bar.get_width()/2., height + 0.01,
#                   f'{height:.3f}', ha='center', va='bottom', fontsize=8)

# # 3. –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
# bars = axes[1,0].bar(results_df['model'], results_df['cv_r2_mean'], 
#               yerr=results_df['cv_r2_std'], capsize=5, alpha=0.7, color='purple', edgecolor='black')
# axes[1,0].set_xlabel('–ú–æ–¥–µ–ª–∏')
# axes[1,0].set_ylabel('CV R2 Score')
# axes[1,0].set_title('–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è R2 (mean ¬± std)')
# axes[1,0].set_xticklabels(results_df['model'], rotation=45, ha='right')
# axes[1,0].grid(True, alpha=0.3)

# # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
# for bar in bars:
#     height = bar.get_height()
#     axes[1,0].text(bar.get_x() + bar.get_width()/2., height + 0.01,
#                   f'{height:.3f}', ha='center', va='bottom', fontsize=8)

# # 4. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ MAE
# bars = axes[1,1].bar(results_df['model'], results_df['val_mae'], alpha=0.7, color='orange', edgecolor='black')
# axes[1,1].set_xlabel('–ú–æ–¥–µ–ª–∏')
# axes[1,1].set_ylabel('MAE')
# axes[1,1].set_title('–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ (Validation)')
# axes[1,1].set_xticklabels(results_df['model'], rotation=45, ha='right')
# axes[1,1].grid(True, alpha=0.3)

# # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
# for bar in bars:
#     height = bar.get_height()
#     axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 0.01,
#                   f'{height:.3f}', ha='center', va='bottom', fontsize=8)

# plt.tight_layout()
# plt.show()

# # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
# print("\n–§–ò–ù–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï:")
# final_results = []
# best_test_r2 = -np.inf
# best_model_name = None

# for name, model in trained_models.items():
#     model_results = next((r for r in results_list if r['model'] == name), None)
#     if model_results and model_results['is_overfit']:
#         print(f"üö´ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å: {name}")
#         continue
        
#     try:
#         y_test_pred = model.predict(X_test_scaled)
        
#         test_r2 = r2_score(y_test, y_test_pred)
#         test_mae = mean_absolute_error(y_test, y_test_pred)
#         test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
        
#         final_results.append({
#             'model': name,
#             'test_r2': test_r2,
#             'test_mae': test_mae,
#             'test_rmse': test_rmse
#         })
        
#         if test_r2 > best_test_r2:
#             best_test_r2 = test_r2
#             best_model_name = name
            
#     except Exception as e:
#         print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–∏ {name}: {e}")

# if not final_results:
#     raise ValueError("–ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –ø—Ä–æ—à–ª–∞ —Ñ–∏–Ω–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É")

# final_results_df = pd.DataFrame(final_results)
# print(final_results_df.round(4))

# # =============================================================================
# # 2.7. –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ò –í–´–í–û–î–´
# # =============================================================================

# print("\n" + "=" * 60)
# print("2.7. –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ò –í–´–í–û–î–´")
# print("=" * 60)

# # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
# if best_model_name:
#     print(f"\nüéØ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model_name}")
#     best_model = trained_models[best_model_name]
#     y_test_pred_best = best_model.predict(X_test_scaled)
    
#     # –î–µ—Ç–∞–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
#     fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
#     # 1. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –†–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
#     axes[0].scatter(y_test, y_test_pred_best, alpha=0.6, s=50, color='blue')
#     axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
#     axes[0].set_xlabel('–†–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
#     axes[0].set_ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
#     axes[0].set_title(f'{best_model_name}\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –†–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
#     axes[0].grid(True, alpha=0.3)
    
#     # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
#     test_r2_best = r2_score(y_test, y_test_pred_best)
#     test_mae_best = mean_absolute_error(y_test, y_test_pred_best)
#     test_rmse_best = np.sqrt(mean_squared_error(y_test, y_test_pred_best))
    
#     axes[0].text(0.05, 0.95, f'R¬≤ = {test_r2_best:.3f}\nMAE = {test_mae_best:.3f}\nRMSE = {test_rmse_best:.3f}', 
#                 transform=axes[0].transAxes, bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.9),
#                 fontsize=10)
    
#     # 2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
#     errors = y_test - y_test_pred_best
#     axes[1].hist(errors, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
#     axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)
#     axes[1].set_xlabel('–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è')
#     axes[1].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
#     axes[1].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫')
#     axes[1].grid(True, alpha=0.3)
    
#     # 3. –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
#     if hasattr(best_model, 'feature_importances_'):
#         feature_importance = pd.DataFrame({
#             'feature': feature_columns,
#             'importance': best_model.feature_importances_
#         }).sort_values('importance', ascending=False).head(10)
        
#         bars = axes[2].barh(feature_importance['feature'], feature_importance['importance'], 
#                            color='lightgreen', edgecolor='black')
#         axes[2].set_xlabel('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞')
#         axes[2].set_title('–¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
#         axes[2].grid(True, alpha=0.3)
        
#         # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏
#         for bar in bars:
#             width = bar.get_width()
#             axes[2].text(width + 0.01, bar.get_y() + bar.get_height()/2.,
#                         f'{width:.3f}', ha='left', va='center', fontsize=8)
#     else:
#         axes[2].text(0.5, 0.5, '–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n–Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞\n–¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏', 
#                     ha='center', va='center', transform=axes[2].transAxes, fontsize=12)
#         axes[2].set_title('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
    
#     plt.tight_layout()
#     plt.show()

# # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
# print("\n" + "="*50)
# print("–§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ò –í–´–í–û–î–´")
# print("="*50)

# print("üìà –°–í–û–î–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
# print(f"   - –í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ: {len(results_df)}")
# print(f"   - –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {results_df['is_overfit'].sum()}")

# if best_model_name:
#     best_final = final_results_df[final_results_df['model'] == best_model_name].iloc[0]
    
#     print(f"\nüéØ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model_name}")
#     print(f"   - Test R¬≤: {best_final['test_r2']:.3f}")
#     print(f"   - Test MAE: {best_final['test_mae']:.3f}")
#     print(f"   - Test RMSE: {best_final['test_rmse']:.3f}")

# print(f"\nüîç –ö–õ–Æ–ß–ï–í–´–ï –ù–ê–ë–õ–Æ–î–ï–ù–ò–Ø:")
# if best_final['test_r2'] > 0.7:
#     quality = "–í–´–°–û–ö–û–ï"
# elif best_final['test_r2'] > 0.5:
#     quality = "–°–†–ï–î–ù–ï–ï"
# elif best_final['test_r2'] > 0.3:
#     quality = "–ù–ò–ó–ö–û–ï"
# else:
#     quality = "–û–ß–ï–ù–¨ –ù–ò–ó–ö–û–ï"

# print(f"   1. –ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {quality}")
# print(f"   2. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏: {'–í–´–°–û–ö–ê–Ø' if results_df['is_overfit'].sum() == 0 else '–°–†–ï–î–ù–Ø–Ø'}")

# print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
# if best_final['test_r2'] < 0.5:
#     print("   1. –°–æ–±—Ä–∞—Ç—å –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö")
#     print("   2. –£–ª—É—á—à–∏—Ç—å feature engineering") 
#     print("   3. –î–æ–±–∞–≤–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ")
# else:
#     print("   1. –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ")
#     print("   2. –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è")
#     print("   3. –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π")

# print("\n" + "="*50)
# print("–ö–£–†–°–û–í–ê–Ø –†–ê–ë–û–¢–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
# print("="*50)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import xgboost as xgb
import warnings
import pickle
import os
warnings.filterwarnings('ignore')

# =============================================================================
# 2.1. –í–´–ë–û–† –ò –û–ü–ò–°–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê
# =============================================================================

print("=" * 60)
print("2.1. –í–´–ë–û–† –ò –û–ü–ò–°–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê")
print("=" * 60)

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
df = pd.read_csv('kp_all_movies_cleanedd.csv')
print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}")

# –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
print("\n–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
print(df.head())

print("\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–∞–Ω–Ω—ã—Ö:")
print(df.info())

print("\n–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
print(df.describe())

# –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
if 'movie_year' in df.columns:
    print(f"\n–í—Ä–µ–º–µ–Ω–Ω–æ–π –ø–µ—Ä–∏–æ–¥ –æ—Ö–≤–∞—Ç–∞: {df['movie_year'].min()} - {df['movie_year'].max()} –≥–≥.")
    print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏—è–º:")
    decade_counts = (df['movie_year'] // 10 * 10).value_counts().sort_index()
    for decade, count in decade_counts.items():
        print(f"  {decade}-–µ: {count} —Ñ–∏–ª—å–º–æ–≤")

# –ê–Ω–∞–ª–∏–∑ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤
if 'kp_rating' in df.columns:
    print(f"\n–ê–Ω–∞–ª–∏–∑ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –ö–∏–Ω–æ–ü–æ–∏—Å–∫:")
    print(f"  –î–∏–∞–ø–∞–∑–æ–Ω: {df['kp_rating'].min():.2f} - {df['kp_rating'].max():.2f}")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ: {df['kp_rating'].mean():.2f} ¬± {df['kp_rating'].std():.2f}")
    print(f"  –ú–µ–¥–∏–∞–Ω–∞: {df['kp_rating'].median():.2f}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–¥–µ–ª–∞ 2.1
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# 1. –î–∏–∞–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–∏–ª—å–º–æ–≤ –ø–æ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏—è–º
if 'movie_year' in df.columns:
    df_decade = (df['movie_year'] // 10 * 10).value_counts().sort_index()
    bars = axes[0,0].bar(df_decade.index, df_decade.values, color='skyblue', alpha=0.7, edgecolor='black')
    axes[0,0].set_xlabel('–î–µ—Å—è—Ç–∏–ª–µ—Ç–∏–µ')
    axes[0,0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–ª—å–º–æ–≤')
    axes[0,0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∏–ª—å–º–æ–≤ –ø–æ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏—è–º')
    axes[0,0].grid(True, alpha=0.3)
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–ø–∏—Å–∏ –∑–Ω–∞—á–µ–Ω–∏–π
    for bar in bars:
        height = bar.get_height()
        axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 5,
                      f'{int(height)}', ha='center', va='bottom', fontsize=9)

# 2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –ö–∏–Ω–æ–ü–æ–∏—Å–∫
if 'kp_rating' in df.columns:
    axes[0,1].hist(df['kp_rating'].dropna(), bins=30, color='lightgreen', alpha=0.7, edgecolor='black')
    axes[0,1].set_xlabel('–†–µ–π—Ç–∏–Ω–≥ –ö–∏–Ω–æ–ü–æ–∏—Å–∫')
    axes[0,1].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    axes[0,1].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –ö–∏–Ω–æ–ü–æ–∏—Å–∫')
    axes[0,1].grid(True, alpha=0.3)

# 3. –ê–Ω–∞–ª–∏–∑ –∂–∞–Ω—Ä–æ–≤
if 'genres' in df.columns:
    all_genres = []
    for genres in df['genres'].dropna():
        if isinstance(genres, str):
            all_genres.extend([genre.strip() for genre in genres.split(',')])
    top_genres = pd.Series(all_genres).value_counts().head(10)
    bars = axes[1,0].barh(range(len(top_genres)), top_genres.values, color='lightcoral', alpha=0.7, edgecolor='black')
    axes[1,0].set_yticks(range(len(top_genres)))
    axes[1,0].set_yticklabels(top_genres.index)
    axes[1,0].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–ª—å–º–æ–≤')
    axes[1,0].set_title('–¢–æ–ø-10 –∂–∞–Ω—Ä–æ–≤')
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–ø–∏—Å–∏ –∑–Ω–∞—á–µ–Ω–∏–π
    for i, bar in enumerate(bars):
        width = bar.get_width()
        axes[1,0].text(width + 5, bar.get_y() + bar.get_height()/2.,
                      f'{int(width)}', ha='left', va='center', fontsize=9)

# 4. –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω
if 'countries' in df.columns:
    all_countries = []
    for countries in df['countries'].dropna():
        if isinstance(countries, str):
            all_countries.extend([country.strip() for country in countries.split(',')])
    top_countries = pd.Series(all_countries).value_counts().head(10)
    bars = axes[1,1].barh(range(len(top_countries)), top_countries.values, color='gold', alpha=0.7, edgecolor='black')
    axes[1,1].set_yticks(range(len(top_countries)))
    axes[1,1].set_yticklabels(top_countries.index)
    axes[1,1].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–ª—å–º–æ–≤')
    axes[1,1].set_title('–¢–æ–ø-10 —Å—Ç—Ä–∞–Ω –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞')
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–ø–∏—Å–∏ –∑–Ω–∞—á–µ–Ω–∏–π
    for i, bar in enumerate(bars):
        width = bar.get_width()
        axes[1,1].text(width + 5, bar.get_y() + bar.get_height()/2.,
                      f'{int(width)}', ha='left', va='center', fontsize=9)

plt.tight_layout()
plt.show()

# =============================================================================
# 2.2. –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –ò –†–ê–ó–í–ï–î–û–ß–ù–´–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• (EDA)
# =============================================================================

print("\n" + "=" * 60)
print("2.2. –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –ò –†–ê–ó–í–ï–î–û–ß–ù–´–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• (EDA)")
print("=" * 60)

# –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
print("\n–ê–ù–ê–õ–ò–ó –ü–†–û–ü–£–©–ï–ù–ù–´–• –ó–ù–ê–ß–ï–ù–ò–ô:")
missing_data = df.isnull().sum()
missing_percent = (df.isnull().sum() / len(df)) * 100
missing_info = pd.DataFrame({
    '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤': missing_data,
    '–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤': missing_percent
})
missing_info = missing_info[missing_info['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤'] > 0]
print(missing_info)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
if len(missing_info) > 0:
    plt.figure(figsize=(12, 6))
    missing_plot_data = missing_info.sort_values('–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤', ascending=False)
    bars = plt.bar(missing_plot_data.index, missing_plot_data['–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤'], 
                  color='salmon', alpha=0.7, edgecolor='black')
    plt.xticks(rotation=45, ha='right')
    plt.xlabel('–ü—Ä–∏–∑–Ω–∞–∫–∏')
    plt.ylabel('–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤ (%)')
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º')
    plt.grid(True, alpha=0.3)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plt.show()
else:
    print("–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")

# –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
def preprocess_data_improved(df):
    df_clean = df.copy()
    
    # –ë–∞–∑–æ–≤—ã–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤
    numeric_columns = ['kp_rating', 'movie_duration', 'kp_rating_count', 'movie_year', 
                     'imdb_rating', 'imdb_rating_count', 'critics_rating']
    
    for col in numeric_columns:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].fillna(df_clean[col].median())
    
    # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±—é–¥–∂–µ—Ç–∞
    def parse_budget_simple(budget_str):
        if pd.isna(budget_str):
            return np.nan
        try:
            budget_str = str(budget_str).replace(' ', '').replace('$', '').replace('‚Ç¨', '').replace('¬£', '')
            budget_str = budget_str.replace('¬•', '').replace('INR', '').replace('CAD', '').replace('AUD', '')
            budget_str = budget_str.replace('–º–ª–Ω', '').replace('million', '')
            return float(budget_str) * 1000000 if any(x in str(budget_str).lower() for x in ['–º–ª–Ω', 'million']) else float(budget_str)
        except:
            return np.nan
    
    if 'budget' in df_clean.columns:
        df_clean['budget_parsed'] = df_clean['budget'].apply(parse_budget_simple)
        df_clean['budget_parsed'] = df_clean['budget_parsed'].fillna(df_clean['budget_parsed'].median())
        df_clean['log_budget'] = np.log1p(df_clean['budget_parsed'])
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –∂–∞–Ω—Ä—ã
    main_genres = ['–¥—Ä–∞–º–∞', '–∫–æ–º–µ–¥–∏—è', '–±–æ–µ–≤–∏–∫', '—Ç—Ä–∏–ª–ª–µ—Ä', '–º–µ–ª–æ–¥—Ä–∞–º–∞', '—Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞', '—É–∂–∞—Å—ã']
    if 'genres' in df_clean.columns:
        for genre in main_genres:
            df_clean[f'genre_{genre}'] = df_clean['genres'].apply(
                lambda x: 1 if isinstance(x, str) and genre in x.lower() else 0
            )
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç—Ä–∞–Ω—ã
    main_countries = ['–°–®–ê', '–í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏—è', '–§—Ä–∞–Ω—Ü–∏—è', '–ì–µ—Ä–º–∞–Ω–∏—è']
    if 'countries' in df_clean.columns:
        for country in main_countries:
            df_clean[f'country_{country}'] = df_clean['countries'].apply(
                lambda x: 1 if isinstance(x, str) and country in x else 0
            )
    
    # –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    df_clean['has_critics_rating'] = (~df_clean['critics_rating'].isna()).astype(int)
    if 'kp_rating_count' in df_clean.columns:
        df_clean['log_kp_rating_count'] = np.log1p(df_clean['kp_rating_count'])
    if 'imdb_rating_count' in df_clean.columns:
        df_clean['log_imdb_rating_count'] = np.log1p(df_clean['imdb_rating_count'])
    
    # –£–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é
    columns_to_drop = ['name_rus', 'name_eng', 'genres', 'countries', 'budget', 
                      'movie_id', 'kp_rating_count', 'imdb_rating_count', 'budget_parsed']
    for col in columns_to_drop:
        if col in df_clean.columns:
            df_clean = df_clean.drop(col, axis=1)
    
    return df_clean

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
df_processed = preprocess_data_improved(df)
print(f"–†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {df_processed.shape}")
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(df_processed.columns)}")
print(f"–ü—Ä–∏–∑–Ω–∞–∫–∏: {list(df_processed.columns)}")

# EDA –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# 1. –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
numeric_columns = df_processed.select_dtypes(include=[np.number]).columns
if len(numeric_columns) > 1:
    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 8 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
    cols_to_plot = numeric_columns[:min(8, len(numeric_columns))]
    correlation_matrix = df_processed[cols_to_plot].corr()
    
    im = axes[0,0].imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)
    axes[0,0].set_xticks(range(len(cols_to_plot)))
    axes[0,0].set_yticks(range(len(cols_to_plot)))
    axes[0,0].set_xticklabels([col[:15] + '...' if len(col) > 15 else col for col in cols_to_plot], 
                             rotation=45, ha='right', fontsize=9)
    axes[0,0].set_yticklabels([col[:15] + '...' if len(col) > 15 else col for col in cols_to_plot], 
                             fontsize=9)
    axes[0,0].set_title('–ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
    for i in range(len(cols_to_plot)):
        for j in range(len(cols_to_plot)):
            text_color = 'white' if abs(correlation_matrix.iloc[i, j]) > 0.5 else 'black'
            axes[0,0].text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',
                         ha="center", va="center", color=text_color, fontsize=8)
    
    plt.colorbar(im, ax=axes[0,0])

# 2. Box-plot –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ñ–∏–ª—å–º–æ–≤
if 'movie_duration' in df_processed.columns:
    data_to_plot = df_processed['movie_duration'].dropna()
    if len(data_to_plot) > 0:
        box_plot = axes[0,1].boxplot(data_to_plot, patch_artist=True)
        box_plot['boxes'][0].set_facecolor('lightblue')
        box_plot['boxes'][0].set_alpha(0.7)
        axes[0,1].set_ylabel('–ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–º–∏–Ω—É—Ç—ã)')
        axes[0,1].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ñ–∏–ª—å–º–æ–≤')
        axes[0,1].grid(True, alpha=0.3)

# 3. Scatter plot: –≥–æ–¥ –≤—ã–ø—É—Å–∫–∞ vs —Ä–µ–π—Ç–∏–Ω–≥
if 'movie_year' in df_processed.columns and 'kp_rating' in df_processed.columns:
    valid_data = df_processed[['movie_year', 'kp_rating']].dropna()
    if len(valid_data) > 0:
        axes[1,0].scatter(valid_data['movie_year'], valid_data['kp_rating'], alpha=0.5, s=20)
        axes[1,0].set_xlabel('–ì–æ–¥ –≤—ã–ø—É—Å–∫–∞')
        axes[1,0].set_ylabel('KP –†–µ–π—Ç–∏–Ω–≥')
        axes[1,0].set_title('–†–µ–π—Ç–∏–Ω–≥ –ø–æ –≥–æ–¥–∞–º')
        axes[1,0].grid(True, alpha=0.3)

# 4. Scatter plot: IMDB vs KP —Ä–µ–π—Ç–∏–Ω–≥
if 'imdb_rating' in df_processed.columns and 'kp_rating' in df_processed.columns:
    valid_data = df_processed[['imdb_rating', 'kp_rating']].dropna()
    if len(valid_data) > 0:
        axes[1,1].scatter(valid_data['imdb_rating'], valid_data['kp_rating'], alpha=0.5, s=20)
        axes[1,1].set_xlabel('IMDB –†–µ–π—Ç–∏–Ω–≥')
        axes[1,1].set_ylabel('KP –†–µ–π—Ç–∏–Ω–≥')
        axes[1,1].set_title('KP vs IMDB —Ä–µ–π—Ç–∏–Ω–≥')
        axes[1,1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# =============================================================================
# 2.3-2.4. –í–´–ë–û–† –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í –ò –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –°–ò–°–¢–ï–ú–´
# =============================================================================

print("\n" + "=" * 60)
print("2.3-2.4. –í–´–ë–û–† –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í –ò –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –°–ò–°–¢–ï–ú–´")
print("=" * 60)

print("""
–í–´–ë–†–ê–ù–ù–´–ï –ò–ù–°–¢–†–£–ú–ï–ù–¢–´:
‚Ä¢ Pandas, NumPy - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
‚Ä¢ Scikit-learn - –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã ML
‚Ä¢ XGBoost - –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥
‚Ä¢ Matplotlib, Seaborn - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è

–ê–†–•–ò–¢–ï–ö–¢–£–†–ê –°–ò–°–¢–ï–ú–´:
1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞
3. Feature engineering
4. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –≤—ã–±–æ—Ä–∫–∏
5. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
6. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
7. –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ—Ü–µ–Ω–∫–∞
8. –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
""")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
fig, ax = plt.subplots(figsize=(12, 6))
stages = ["–ó–∞–≥—Ä—É–∑–∫–∞\n–¥–∞–Ω–Ω—ã—Ö", "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞", "Feature\nEngineering", 
          "–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ\n–¥–∞–Ω–Ω—ã—Ö", "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ", "–û–±—É—á–µ–Ω–∏–µ\n–º–æ–¥–µ–ª–µ–π", 
          "–í–∞–ª–∏–¥–∞—Ü–∏—è", "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è"]

y_pos = np.arange(len(stages))
colors = plt.cm.Set3(np.linspace(0, 1, len(stages)))

bars = ax.barh(y_pos, [1]*len(stages), color=colors, alpha=0.7, edgecolor='black')
ax.set_yticks(y_pos)
ax.set_yticklabels(stages, fontsize=10)
ax.set_xlabel('–≠—Ç–∞–ø—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏')
ax.set_title('–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è')
ax.grid(True, alpha=0.3)

# –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–µ–ª–∫–∏
for i in range(len(stages)-1):
    ax.annotate('‚Üí', xy=(0.5, y_pos[i] - 0.4), xytext=(0.5, y_pos[i] - 0.1),
                arrowprops=dict(arrowstyle='->', lw=2, color='black'),
                ha='center', va='center', fontsize=16, xycoords='data')

plt.tight_layout()
plt.show()

# =============================================================================
# 2.5. –†–ï–ê–õ–ò–ó–ê–¶–ò–Ø –ò –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò
# =============================================================================

print("\n" + "=" * 60)
print("2.5. –†–ï–ê–õ–ò–ó–ê–¶–ò–Ø –ò –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò")
print("=" * 60)

# –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã—Å–æ–∫–æ–∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
def remove_highly_correlated_features(df, threshold=0.85):
    corr_matrix = df.corr().abs()
    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]
    if to_drop:
        print(f"–£–¥–∞–ª—è–µ–º –≤—ã—Å–æ–∫–æ–∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {to_drop}")
        return df.drop(to_drop, axis=1)
    else:
        print("–í—ã—Å–æ–∫–æ–∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
        return df

df_processed = remove_highly_correlated_features(df_processed, threshold=0.8)
print(f"–†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {df_processed.shape}")

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
if 'kp_rating' not in df_processed.columns:
    raise ValueError("–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'kp_rating' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞–Ω–Ω—ã—Ö")

feature_columns = [col for col in df_processed.columns if col != 'kp_rating']
X = df_processed[feature_columns]
y = df_processed['kp_rating']

print(f"–§–∏–Ω–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_columns)}")
print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å X: {X.shape}, y: {y.shape}")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
if len(X) == 0 or len(y) == 0:
    raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
try:
    y_bins = pd.cut(y, bins=min(5, len(y.unique())), labels=False)
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y_bins
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.25, random_state=42, 
        stratify=pd.cut(y_temp, bins=min(5, len(y_temp.unique())), labels=False)
    )
except ValueError as e:
    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏: {e}")
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –µ—Å–ª–∏ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)

print(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape}")
print(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_val.shape}")
print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape}")

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# –§—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏
def evaluate_model_improved(model, X_train, X_val, y_train, y_val, model_name):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""
    
    try:
        cv_scores = cross_val_score(model, X_train, y_train, cv=min(5, len(y_train)), scoring='r2')
    except:
        cv_scores = [0]  # –ï—Å–ª–∏ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
        
    model.fit(X_train, y_train)
    y_train_pred = model.predict(X_train)
    y_val_pred = model.predict(X_val)
    
    train_r2 = r2_score(y_train, y_train_pred)
    val_r2 = r2_score(y_val, y_val_pred)
    train_mae = mean_absolute_error(y_train, y_train_pred)
    val_mae = mean_absolute_error(y_val, y_val_pred)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
    
    r2_gap = train_r2 - val_r2
    relative_overfit = (train_r2 - val_r2) / abs(train_r2) if train_r2 != 0 else 0
    
    results = {
        'model': model_name,
        'cv_r2_mean': np.mean(cv_scores),
        'cv_r2_std': np.std(cv_scores),
        'train_r2': train_r2,
        'val_r2': val_r2,
        'r2_gap': r2_gap,
        'relative_overfit': relative_overfit,
        'train_mae': train_mae,
        'val_mae': val_mae,
        'train_rmse': train_rmse,
        'val_rmse': val_rmse,
        'is_overfit': relative_overfit > 0.3 and r2_gap > 0.1
    }
    
    return results, model

# –ú–æ–¥–µ–ª–∏ —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
models_regularized = {
    'Ridge': Ridge(alpha=10.0, random_state=42),
    'Lasso': Lasso(alpha=1.0, random_state=42, max_iter=2000),
    'ElasticNet': ElasticNet(alpha=0.5, l1_ratio=0.5, random_state=42, max_iter=2000),
    'RandomForest': RandomForestRegressor(
        n_estimators=50,
        max_depth=8,
        min_samples_split=20,
        min_samples_leaf=10,
        max_features='sqrt',
        random_state=42,
        n_jobs=-1
    ),
    'GradientBoosting': GradientBoostingRegressor(
        n_estimators=100,
        max_depth=4,
        learning_rate=0.05,
        min_samples_split=15,
        min_samples_leaf=10,
        subsample=0.8,
        random_state=42
    ),
    'XGBoost': xgb.XGBRegressor(
        n_estimators=100,
        max_depth=4,
        learning_rate=0.05,
        reg_alpha=1.0,
        reg_lambda=1.0,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1
    )
}

print("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π...")
results_list = []
trained_models = {}

for name, model in models_regularized.items():
    print(f"–û–±—É—á–µ–Ω–∏–µ {name}...")
    try:
        results, trained_model = evaluate_model_improved(
            model, X_train_scaled, X_val_scaled, y_train, y_val, name
        )
        results_list.append(results)
        trained_models[name] = trained_model
        status = "‚ö†Ô∏è –ü–ï–†–ï–û–ë–£–ß–ï–ù–ê" if results['is_overfit'] else "‚úÖ –ù–û–†–ú–ê"
        print(f"  {status} | Train R2: {results['train_r2']:.3f} | Val R2: {results['val_r2']:.3f}")
    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")

# =============================================================================
# 2.6. –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ú–û–î–ï–õ–ò
# =============================================================================

print("\n" + "=" * 60)
print("2.6. –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ú–û–î–ï–õ–ò")
print("=" * 60)

if not results_list:
    raise ValueError("–ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞")

# –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
results_df = pd.DataFrame(results_list)
print("\n–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ú–û–î–ï–õ–ï–ô:")
display_cols = ['model', 'train_r2', 'val_r2', 'r2_gap', 'relative_overfit', 'is_overfit']
print(results_df[display_cols].round(4))

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ R2
colors = ['red' if overfit else 'green' for overfit in results_df['is_overfit']]
x_pos = np.arange(len(results_df))

bars1 = axes[0,0].bar(x_pos - 0.2, results_df['train_r2'], 0.4, label='Train R2', alpha=0.7, color='lightblue')
bars2 = axes[0,0].bar(x_pos + 0.2, results_df['val_r2'], 0.4, label='Val R2', alpha=0.7, color=colors)
axes[0,0].set_xlabel('–ú–æ–¥–µ–ª–∏')
axes[0,0].set_ylabel('R2 Score')
axes[0,0].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ R2 (–ö—Ä–∞—Å–Ω—ã–π = –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ)')
axes[0,0].set_xticks(x_pos)
axes[0,0].set_xticklabels(results_df['model'], rotation=45, ha='right')
axes[0,0].legend()
axes[0,0].grid(True, alpha=0.3)

# –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                      f'{height:.3f}', ha='center', va='bottom', fontsize=8)

# 2. Gap –∞–Ω–∞–ª–∏–∑
bars = axes[0,1].bar(results_df['model'], results_df['r2_gap'], color=colors, alpha=0.7, edgecolor='black')
axes[0,1].axhline(y=0.1, color='red', linestyle='--', label='–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ø–æ—Ä–æ–≥')
axes[0,1].set_xlabel('–ú–æ–¥–µ–ª–∏')
axes[0,1].set_ylabel('R2 Gap (Train - Val)')
axes[0,1].set_title('–†–∞–∑—Ä—ã–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏')
axes[0,1].set_xticklabels(results_df['model'], rotation=45, ha='right')
axes[0,1].legend()
axes[0,1].grid(True, alpha=0.3)

# –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
for bar in bars:
    height = bar.get_height()
    axes[0,1].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                  f'{height:.3f}', ha='center', va='bottom', fontsize=8)

# 3. –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
bars = axes[1,0].bar(results_df['model'], results_df['cv_r2_mean'], 
              yerr=results_df['cv_r2_std'], capsize=5, alpha=0.7, color='purple', edgecolor='black')
axes[1,0].set_xlabel('–ú–æ–¥–µ–ª–∏')
axes[1,0].set_ylabel('CV R2 Score')
axes[1,0].set_title('–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è R2 (mean ¬± std)')
axes[1,0].set_xticklabels(results_df['model'], rotation=45, ha='right')
axes[1,0].grid(True, alpha=0.3)

# –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
for bar in bars:
    height = bar.get_height()
    axes[1,0].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                  f'{height:.3f}', ha='center', va='bottom', fontsize=8)

# 4. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ MAE
bars = axes[1,1].bar(results_df['model'], results_df['val_mae'], alpha=0.7, color='orange', edgecolor='black')
axes[1,1].set_xlabel('–ú–æ–¥–µ–ª–∏')
axes[1,1].set_ylabel('MAE')
axes[1,1].set_title('–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ (Validation)')
axes[1,1].set_xticklabels(results_df['model'], rotation=45, ha='right')
axes[1,1].grid(True, alpha=0.3)

# –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
for bar in bars:
    height = bar.get_height()
    axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                  f'{height:.3f}', ha='center', va='bottom', fontsize=8)

plt.tight_layout()
plt.show()

# –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
print("\n–§–ò–ù–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï:")
final_results = []
best_test_r2 = -np.inf
best_model_name = None

for name, model in trained_models.items():
    model_results = next((r for r in results_list if r['model'] == name), None)
    if model_results and model_results['is_overfit']:
        print(f"üö´ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å: {name}")
        continue
        
    try:
        y_test_pred = model.predict(X_test_scaled)
        
        test_r2 = r2_score(y_test, y_test_pred)
        test_mae = mean_absolute_error(y_test, y_test_pred)
        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
        
        final_results.append({
            'model': name,
            'test_r2': test_r2,
            'test_mae': test_mae,
            'test_rmse': test_rmse
        })
        
        if test_r2 > best_test_r2:
            best_test_r2 = test_r2
            best_model_name = name
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–∏ {name}: {e}")

if not final_results:
    raise ValueError("–ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –ø—Ä–æ—à–ª–∞ —Ñ–∏–Ω–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É")

final_results_df = pd.DataFrame(final_results)
print(final_results_df.round(4))

# =============================================================================
# 2.7. –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ò –í–´–í–û–î–´
# =============================================================================

print("\n" + "=" * 60)
print("2.7. –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ò –í–´–í–û–î–´")
print("=" * 60)

# –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
if best_model_name:
    print(f"\nüéØ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model_name}")
    best_model = trained_models[best_model_name]
    y_test_pred_best = best_model.predict(X_test_scaled)
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    # 1. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –†–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    axes[0].scatter(y_test, y_test_pred_best, alpha=0.6, s=50, color='blue')
    axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    axes[0].set_xlabel('–†–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
    axes[0].set_ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
    axes[0].set_title(f'{best_model_name}\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –†–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
    axes[0].grid(True, alpha=0.3)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    test_r2_best = r2_score(y_test, y_test_pred_best)
    test_mae_best = mean_absolute_error(y_test, y_test_pred_best)
    test_rmse_best = np.sqrt(mean_squared_error(y_test, y_test_pred_best))
    
    axes[0].text(0.05, 0.95, f'R¬≤ = {test_r2_best:.3f}\nMAE = {test_mae_best:.3f}\nRMSE = {test_rmse_best:.3f}', 
                transform=axes[0].transAxes, bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.9),
                fontsize=10)
    
    # 2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
    errors = y_test - y_test_pred_best
    axes[1].hist(errors, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
    axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)
    axes[1].set_xlabel('–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è')
    axes[1].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    axes[1].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫')
    axes[1].grid(True, alpha=0.3)
    
    # 3. –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    if hasattr(best_model, 'feature_importances_'):
        feature_importance = pd.DataFrame({
            'feature': feature_columns,
            'importance': best_model.feature_importances_
        }).sort_values('importance', ascending=False).head(10)
        
        bars = axes[2].barh(feature_importance['feature'], feature_importance['importance'], 
                           color='lightgreen', edgecolor='black')
        axes[2].set_xlabel('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞')
        axes[2].set_title('–¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
        axes[2].grid(True, alpha=0.3)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏
        for bar in bars:
            width = bar.get_width()
            axes[2].text(width + 0.01, bar.get_y() + bar.get_height()/2.,
                        f'{width:.3f}', ha='left', va='center', fontsize=8)
    else:
        axes[2].text(0.5, 0.5, '–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n–Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞\n–¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏', 
                    ha='center', va='center', transform=axes[2].transAxes, fontsize=12)
        axes[2].set_title('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
    
    plt.tight_layout()
    plt.show()

# –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
print("\n" + "="*50)
print("–§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ò –í–´–í–û–î–´")
print("="*50)

print("üìà –°–í–û–î–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
print(f"   - –í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ: {len(results_df)}")
print(f"   - –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {results_df['is_overfit'].sum()}")

if best_model_name:
    best_final = final_results_df[final_results_df['model'] == best_model_name].iloc[0]
    
    print(f"\nüéØ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model_name}")
    print(f"   - Test R¬≤: {best_final['test_r2']:.3f}")
    print(f"   - Test MAE: {best_final['test_mae']:.3f}")
    print(f"   - Test RMSE: {best_final['test_rmse']:.3f}")

print(f"\nüîç –ö–õ–Æ–ß–ï–í–´–ï –ù–ê–ë–õ–Æ–î–ï–ù–ò–Ø:")
if best_final['test_r2'] > 0.7:
    quality = "–í–´–°–û–ö–û–ï"
elif best_final['test_r2'] > 0.5:
    quality = "–°–†–ï–î–ù–ï–ï"
elif best_final['test_r2'] > 0.3:
    quality = "–ù–ò–ó–ö–û–ï"
else:
    quality = "–û–ß–ï–ù–¨ –ù–ò–ó–ö–û–ï"

print(f"   1. –ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {quality}")
print(f"   2. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏: {'–í–´–°–û–ö–ê–Ø' if results_df['is_overfit'].sum() == 0 else '–°–†–ï–î–ù–Ø–Ø'}")

print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
if best_final['test_r2'] < 0.5:
    print("   1. –°–æ–±—Ä–∞—Ç—å –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö")
    print("   2. –£–ª—É—á—à–∏—Ç—å feature engineering") 
    print("   3. –î–æ–±–∞–≤–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ")
else:
    print("   1. –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ")
    print("   2. –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è")
    print("   3. –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π")

# =============================================================================
# –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò –î–õ–Ø –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø –í –†–ï–ö–û–ú–ï–ù–î–ê–¢–ï–õ–¨–ù–û–ô –°–ò–°–¢–ï–ú–ï
# =============================================================================

print("\n" + "="*60)
print("–°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò –î–õ–Ø –†–ï–ö–û–ú–ï–ù–î–ê–¢–ï–õ–¨–ù–û–ô –°–ò–°–¢–ï–ú–´")
print("="*60)

def save_trained_model(best_model, scaler, feature_columns, file_path='trained_movie_model.pkl'):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ"""
    model_data = {
        'model': best_model,
        'scaler': scaler,
        'feature_columns': feature_columns,
        'model_type': best_model_name,
        'test_r2': best_final['test_r2'],
        'test_mae': best_final['test_mae'],
        'test_rmse': best_final['test_rmse']
    }
    
    with open(file_path, 'wb') as f:
        pickle.dump(model_data, f)
    print(f"‚úÖ –ú–æ–¥–µ–ª—å {best_model_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {file_path}")
    print(f"üìä –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏: R¬≤={best_final['test_r2']:.3f}, MAE={best_final['test_mae']:.3f}")
    return model_data

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
if best_model_name:
    model_data = save_trained_model(best_model, scaler, feature_columns)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏
    all_models_data = {
        'best_model': best_model,
        'best_model_name': best_model_name,
        'all_models': trained_models,
        'scaler': scaler,
        'feature_columns': feature_columns,
        'test_results': final_results_df,
        'feature_importance': feature_importance if 'feature_importance' in locals() else None
    }
    
    with open('all_trained_models.pkl', 'wb') as f:
        pickle.dump(all_models_data, f)
    print("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ all_trained_models.pkl")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
    data_info = {
        'original_shape': df.shape,
        'processed_shape': df_processed.shape,
        'feature_names': feature_columns,
        'target_name': 'kp_rating',
        'data_preprocessing_info': '–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å feature engineering'
    }
    
    with open('model_data_info.pkl', 'wb') as f:
        pickle.dump(data_info, f)
    print("‚úÖ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ model_data_info.pkl")

print("\n" + "="*50)
print("üéâ –ö–£–†–°–û–í–ê–Ø –†–ê–ë–û–¢–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
print("="*50)
print("üìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
print("   - trained_movie_model.pkl (–ª—É—á—à–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã)")
print("   - all_trained_models.pkl (–≤—Å–µ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏)")
print("   - model_data_info.pkl (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö)")
print("\nüöÄ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É!")